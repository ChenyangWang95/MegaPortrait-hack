institutetext: Samsung AI Center – Moscow
institutetext: Skolkovo Institute of Science and Technology
institutetext: Yandex Armenia
https://samsunglabs.github.io/rome/
Realistic One-shot Mesh-based Head Avatars
Taras Khakhulin
1122
Vanessa Sklyarova
1122

Victor Lempitsky
33
Egor Zakharov
1122
Abstract
We present a system for realistic one-shot mesh-based human head avatars creation, ROME for short. Using a single photograph, our model estimates a person-specific head mesh and the associated neural texture, which encodes both local photometric and geometric details. The resulting avatars are rigged and can be rendered using a neural network, which is trained alongside the mesh and texture estimators on a dataset of in-the-wild videos. In the experiments, we observe that our system performs competitively both in terms of head geometry recovery and the quality of renders, especially for the cross-person reenactment.

1Introduction
Personalized human avatars are becoming a key technology across several application domains, such as telepresence, virtual worlds, and online commerce. In many practical cases, it is sufficient to personalize only a part of the avatar’s body, while the remaining areas can then be picked from a pre-defined library of assets or omitted from the interface. Towards this end, many applications require personalization at the head level, i.e., the creation of person-specific head models, thus making it an important and viable intermediate step between personalizing only the face and the entire body. Alone, face personalization is often insufficient, while the full-body reconstruction remains a complicated task and leads to the reduced quality of the models or requires cumbersome data collection.

Acquiring human avatars from a single photograph (in a “one-shot” setting) offers the highest convenience for the end-user. However, their creation process is particularly challenging and requires strong priors on human geometry and appearance. To this end, parametric models are long known to offer good personalization solutions [1] and were recently adapted to one-shot performance [2, 3, 4]. Such models can be learned from a relatively small dataset of 3D scans and represent geometry and appearance via textured meshes, making them compatible with many computer graphics applications and pipelines. However, they cannot be trivially expanded to the whole head region due to the large geometric variability of the non-facial parts such as hair and neck.

Our proposed system addresses this issue and allows parametric face models to represent the non-facial parts. In order to handle the increased geometric and photometric variability, we train our method on a large dataset of in-the-wild videos [5] and use neural networks to parameterize both the geometry and the appearance. For the appearance modeling, we follow the deferred neural rendering [6] paradigm and employ a combination of neural textures and rendering networks. In addition, a neural rendering framework [7] is used to enable end-to-end training and achieve high visual realism of the resulting head models. After training, the geometric and appearance networks can be conditioned on the information extracted from a single photograph, enabling one-shot realistic avatar generation.

To the best of our knowledge, our system is the first that is capable of creating realistic personalized human head models in a rigged mesh format from a single photograph. This distinguishes our model from a growing class of approaches that a) recover neural head avatars without explicit geometry [8, 9, 10, 11], b) can personalize the face region but not the whole head [12, 13, 1, 2], and c) from commercial systems that create non-photorealistic mesh avatars from a single image [14, 15]. Alongside our main model, we also discuss its simplified version based on a linear blendshapes basis and show how to train it using the same video dataset. Below, we refer to the avatars generated by our system as ROME avatars (Realistic One-shot Mesh-based avatars).

Refer to caption	Refer to caption	Refer to caption	Refer to caption	 Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	 Refer to caption	Refer to caption	Refer to caption	Refer to caption
Source	Driver	Mesh	Render	Source	Driver	Mesh	Render
Figure 1:Our system creates realistic mesh-based avatars from a single source photo. These avatars are rigged, i.e., they can be driven by the animation parameters from a different driving frame. At the same time, our obtained meshes and renderes achieve a high degree of personalization in both appearance and geometry and are trained in an end-to-end fashion on a dataset of in-the-wild videos without any additional 3D supervision.
2Related work
Parametric models of human faces. Over the recent decades, 3D face reconstruction methods have been actively employed to tackle the problems of face tracking and alignment [16, 3], face recognition [17, 18], and generative modelling [12, 13, 19, 20, 21, 22]. In all these scenarios, statistical mesh-based models, aka parametric models [1], remain one of the widely used tools [23, 24]. State-of-the-art parametric models for human heads consist of rigged meshes [25] which support a diverse range of animations via disentangled shape and expression blendshapes and rigid motions for the jaw, neck, and eyeballs. However, they only provide reconstructions for the face, ears, neck, and forehead regions, limiting the range of applications. Including full head reconstruction (i.e., hair and shoulders) into these parametric models is possible, but existing approaches require significantly more training data to be gathered in the form of 3D scans. Instead, in our work, we propose to leverage existing large-scale datasets [5] of in-the-wild videos via the learning-by-synthesis paradigm without any additional 3D annotations.

Neural 3D human head models. While parametric models provide sufficient reconstruction quality for many downstream applications, they are not able to depict the fine appearance details that are needed for photorealistic modelling. In recent years, the problem of representing complex geometry and appearance of humans started being addressed using high-capacity deep neural networks. Some of these works use strong human-specific priors [20, 2, 26, 27], while others fit high-capacity networks to data without the use of such priors [28, 29, 19, 22, 21, 30, 31]. The latter methods additionally differ by the type of data structure used to represent the geometry, namely, mesh-based [2, 22, 21, 32], point-based [28, 33], and implicit models [19, 29, 20, 26, 31, 27, 34]. Additionally, recently there have emerged the hybrid models [35, 36] where authors integrate face priors from parametric models with implicit representations to learn geometry and rendering for the specific person from the video.

However, mesh-based models arguably represent the most convenient class of methods for downstream applications. They provide better rendering quality and better temporal stability than point-based neural rendering. Also, unlike methods based on implicit geometry, mesh-based methods preserve topology and rigging capabilities and are much faster during fitting and rendering. However, current mesh-based methods either severely limit the range of deformations [2], making it infeasible to learn more complex geometry like hair and clothed shoulders, operate in the multi-shot setting [32] or require 3D scans as training data [22, 21]. Our proposed method is also mesh-based, but we allow the prediction of complex deformations without 3D supervision and using a single image, lifting the limitations of the previous and concurrent works.

One-shot neural head models. Advances in neural networks also led to the development of methods that directly predict images using large ConvNets operating in the 2D image domain, with effectively no underlying 3D geometry [9, 10, 11] or very coarse 3D geometry [8]. These methods achieve state-of-the-art realism [8], use in-the-wild images or videos with no 3D annotations for training, and can create avatars from a single image. However, the lack of an explicit geometric model makes these models incompatible with many real-world applications and limits the span of camera poses that these methods can handle.

Refer to caption
Figure 2: Overview of our approach and the detailed scheme of the head mesh reconstruction. Given the source photo, we first estimate a neural texture that encodes local geometric and photometric details of visible and occluded parts. We then use a pre-trained system [2] for face reconstruction to estimate an initial mesh with a reconstructed facial part. We call this step face and 3D pose estimation. During head mesh reconstruction (bottom), using the estimated neural texture and the initial mesh, we predict the offsets for the mesh vertices, which do not correspond to a face. The offsets are predicted with a combination of a convolutional network 
�
geom
 and a perceptron network 
�
geom
. We then render the personalized head mesh using the camera parameters, estimated by a pre-trained regressor [2] while superimposing the predicted neural texture. Finally, the rendering network 
�
img
 estimates the RGB image and the mask from the render.
Neural mesh rendering. Recently, approaches that combine explicit data structures (point clouds or meshes) with neural image generation have emerged. These methods gained popularity thanks to the effectiveness of the pioneering Deferred Neural Rendering system [6], as well as recent advances in differentiable mesh rendering [7, 37, 38]. Neural mesh rendering uses 2D convolutional networks to model complex photometric properties of surfaces. It achieves high realism of renders with fine details present even when they are missing in the underlying geometric model. In this work, we adapt these advances to human head modelling while training using a large dataset of in-the-wild videos.

3Method
Our goal is to build a system that jointly learns to produce photorealistic renders of human heads and estimate their 3D meshes using only a single image without any 3D supervision.

To achieve that, we use a large-scale dataset [5] of in-the-wild videos with talking speakers. All frames in each video are assumed to depict the same person in the same environment (defined by lighting, hairstyle, and person’s clothing). At each training step, we sample two random frames 
𝐱
�
 and 
𝐱
�
 from a random training video. Our goal is to reconstruct and render the target image 
𝐱
^
�
 given a) the personal details and the face shape extracted from the source image 
𝐱
�
, as well as b) the head pose, the facial expression, and the camera pose estimated from the target image 
𝐱
�
. The final reconstruction loss is backpropagated and used to update the parameters of the model’s components.

After training, we can create a personalized head model by estimating all parameters from a single image. This model can then be animated using face tracking parameters extracted from any talking head sequence and rendered from a range of viewpoints similar to those present in the training dataset (Figure 1).

3.1Model overview
In our model, we jointly train multiple neural networks that perform rendering and mesh reconstruction. The training pipeline proceeds as follows (Figure 2):

Neural texture estimation. The source image 
𝐱
�
 is encoded into a neural texture 
𝐓
�
, which describes both person-specific appearance and geometry. The encoding is done by a convolutional neural network 
�
tex
.

Face and 3D pose estimation. In parallel, we apply a pre-trained DECA system [2] for face reconstruction to both the source and the target image, which estimates facial shape, expression, and head pose. Internally, it uses the FLAME parametric head model [25], which includes mesh topology, texture mapping, and blendshapes. We use the face shape from the source image 
𝐱
�
 as well as the facial expression and the camera pose from the target image 
𝐱
�
 for further processing.

Head mesh reconstruction. The vertices of the DECA mesh with personalized face region and generic non-facial parts are rendered into an 
�
​
�
​
�
-coordinate texture using the predefined texture mapping. The 
�
​
�
​
�
-texture and the neural texture 
𝐓
�
 are concatenated and processed with the U-Net network [39] 
�
geom
 into a new texture map 
𝐙
�
, called latent geometry map. The 3D displacements for each mesh vertex are then decoded independently by the multi-layer perceptron 
�
geom
 that predicts a 3D offset 
Δ
​
𝐯
^
 for each vertex. This step reconstructs the personalized model for non-face parts of the head mesh. The obtained reconstructions are compatible with the topology/connectivity of the FLAME mesh [25].

Deferred neural rendering. The personalized head mesh is rendered using the pose estimated by DECA for the target image and with the superimposed neural texture. The rendered neural texture and the rasterized surface normals are concatenated and processed by the decoding (rendering) U-Net network 
�
img
 to predict the rendered image 
𝐱
^
�
 and the segmentation mask 
𝐬
^
�
. During training, the difference between the predictions and the ground truths is used to update all components of our system.

Below we discuss our system and its training process in more detail. We also describe a training procedure for a simplified version of our model, which represents head geometry using a linear basis of blendshapes.

3.2Parametric face modeling
Our method uses a predefined head mesh with the corresponding topology, texture coordinates 
�
, and rigging parameters, which remain fixed for all avatars. More specifically, we use FLAME [25] head model that has 
�
 base vertices 
𝐯
base
∈
ℝ
3
​
�
, and two sets of 
�
 and 
�
 basis vectors (blendshapes) that encode shape 
ℬ
∈
ℝ
3
​
�
×
�
 and expression 
𝒟
∈
ℝ
3
​
�
×
�
. The reconstruction process is carried out in two stages. First, the basis vectors are blended using the person- and expression-specific vectors of linear coefficients 
�
 and 
�
. Then, the linear blend skinning [25] function 
𝒲
 is applied, parameterized by the angles 
�
, which rotates the predefined groups of vertices around linearly estimated joints. The final reconstruction in world coordinates can be expressed as follows:

𝐯
​
(
�
,
�
,
�
)
=
𝒲
​
(
𝐯
base
+
ℬ
​
�
+
𝒟
​
�
,
�
)
.
In previous works [12], a similar set of parameters for the 3DMM [1] parametric model was obtained via photometric optimization. More recently, learning-based methods [2, 3] capable of feed-forward estimation started to emerge. In our work, given an input image, we use a pre-trained feed-forward DECA system [2] to estimate 
�
,
�
, 
�
, and the camera parameters.

During training, we apply DECA to both source image 
𝐱
�
 and the target image 
𝐱
�
. The face shape parameters 
�
�
 from the source image 
𝐱
�
 alongside the expression 
�
�
, head pose 
�
�
 and camera parameters from the target image 
𝐱
�
 are then used to reconstruct the initial FLAME vertices 
𝐯
�
=
𝐯
​
(
�
�
,
�
�
,
�
�
)
, as well as camera transform 
𝒫
�
.

3.3Head mesh reconstruction
The FLAME vertices 
𝐯
�
 estimated by DECA provide good reconstructions for the face region but lack any person-specific details in the remaining parts of the head (hair and shoulders). To alleviate that, we predict person-specific mesh offsets for non-facial regions while preserving the face shape predicted by DECA. We additionally exclude ear regions since their geometry in the initial mesh is too complex to be learned from in-the-wild video datasets.

These mesh offsets are estimated in two steps. First, we encode both the 
�
​
�
​
�
-coordinate texture and the neural texture 
𝐓
�
 into the latent geometry texture map 
𝐙
�
 via a U-Net network 
�
geom
. It allows the produced latent map to contain both positions of the initial vertices 
𝐯
�
 and their semantics, provided by the neural texture.

From 
𝐙
�
 we obtain the vectors 
𝐳
�
�
 by bilinear interpolation at the fixed texture coordinates 
�
. The vectors 
𝐳
�
�
 and their coordinates 
�
 are then concatenated and passed through a multi-layer perceptron 
�
geom
 to predict the coefficients 
𝐦
^
�
∈
ℝ
3
​
�
×
3
 independently for each vertex in the mesh. These coefficients are multiplied elementwise by the normals 
𝐧
�
, calculated for each vertex in 
𝐯
�
, to obtain the displacements: 
Δ
​
𝐯
^
�
=
𝐦
^
⊙
𝐧
�
. These displacements are then zeroed out for face and ear regions, and the final reconstruction in world coordinates is obtained as follows: 
𝐯
^
�
=
𝐯
�
+
Δ
​
𝐯
^
�
.

3.4Deferred neural rendering
We render the reconstructed head vertices 
𝐯
^
�
 using the topology and texture coordinates 
�
 from the FLAME model with the superimposed neural texture 
𝐓
�
. For that, we use a differentiable mesh renderer 
ℛ
 [7] with the camera transform 
𝒫
�
 estimated by DECA for the target image 
𝐱
�
.

The resulting rasterization, which includes both the neural texture and the surface normals, is processed by the rendering network 
�
img
 to obtain the predicted image 
𝐱
^
�
 and the segmentation mask 
𝐬
^
�
. 
�
img
 consists of two U-Nets that separately decode an image and a mask. The result of the deferred neural rendering is the reconstruction of the target image 
𝐱
^
�
 and its mask 
𝐬
^
�
, which are compared to the ground-truth image 
𝐱
�
 and mask 
𝐬
�
 respectively.

3.5Training objectives
In our approach, we learn the geometry of hair and shoulders, which are not reconstructed by the pre-trained DECA estimator, without any ground-truth 3D supervision during training. For that we utilize two types of objectives: segmentation-based geometric losses 
ℒ
geom
 and photometric losses 
ℒ
photo
.

We found that explicitly assigning subsets of mesh vertices to the neck and the hair regions helps a lot with the quality of final deformations. It allows us to introduce a topological prior for the predicted offsets, which is enforced by.

To evaluate the geometric losses, we calculate two separate occupancy masks using a soft rasterization operation [37]. First, 
𝐨
^
�
hair
 is calculated with detached neck vertices, so that the gradient flows through that mask only to the offsets corresponding to the hair vertices, and then 
𝐨
^
�
 is calculated with detached hair vertices. We match the hair occupancy mask to the ground-truth mask 
𝐬
�
hair
 (which covers the hair, face, and ears), and the estimated occupancy mask to the whole segmentation mask 
𝐬
�
: 
ℒ
occ
=
�
hair
​
‖
𝐨
^
�
hair
−
𝐬
�
hair
‖
2
2
+
�
o
​
‖
𝐨
^
�
−
𝐬
�
‖
2
2
.

We also use an auxiliary Chamfer loss to ensure that the predicted mesh vertices cover the head more uniformly. Specifically, we match the 2D coordinates of the mesh vertices projected into the target image to the head segmentation mask. We denote the subset of predicted mesh vertices, visible in the target image, as 
𝐩
^
�
=
𝒫
�
′
​
(
𝐯
^
�
)
, and the number of these vertices as 
�
�
, so that 
𝐩
^
�
∈
𝐑
�
�
×
2
. Notice that operator 
𝒫
�
′
 here not only does the camera transformation but also discards the 
�
 coordinate of the projected mesh vertices. To compute the loss, we then sample 
�
�
 2D points from the segmentation mask 
𝐬
�
 and estimate the Chamfer distance between the sampled set of points 
𝐩
�
 and the vertex projections:

ℒ
chm
=
1
2
​
�
�
​
∑
�
^
�
∈
𝐩
^
�
‖
�
^
�
−
arg
⁡
min
�
∈
𝐩
�
⁡
‖
�
−
�
^
�
‖
‖
+
1
2
​
�
�
​
∑
�
�
∈
𝐩
�
‖
�
�
−
arg
⁡
min
�
^
∈
𝐩
^
�
⁡
‖
�
^
−
�
�
‖
‖
.
Lastly, we regularize the learned geometry using the Laplacian penalty [40]. Initially, we found that regularizing offsets 
Δ
​
𝐯
^
 worked better than regularizing full coordinates 
𝐯
^
 and stuck to that approach for all experiments. Our version of the Laplacian loss can be written as:

ℒ
lap
=
1
�
​
∑
�
=
1
�
‖
Δ
​
𝐯
^
�
−
1
𝒩
​
(
�
)
​
∑
�
∈
𝒩
​
(
�
)
Δ
​
𝐯
^
�
‖
1
,
where 
𝒩
​
(
�
)
 denotes a set indices for vertices adjacent to the 
�
-th vertex in the mesh.

We also use photometric optimization that matches the predicted and the ground truth images. This allows us to obtain photorealistic renders and aid in learning proper geometric reconstructions. We utilize perceptual loss 
ℒ
per
 [41], the face recognition loss 
ℒ
idt
 [42] and adversarial loss 
ℒ
adv
 [43, 44]. We also use the Dice loss 
ℒ
seg
 [45] to match the predicted segmentation masks.

The final objective is weighted sum of the geometric and the photometric losses described above.

3.6Linear deformation model
In addition to the full non-linear model introduced above, we consider a simplified parametric model with a linear basis of offsets 
Δ
​
𝐯
^
. While this model is similar to parametric models [25, 46], we still do not use 3D scans for training and instead obtain our linear model by “distilling” the non-linear model. Additionally, we train a feed-forward estimator that predicts the linear coefficients from the input image.

The motivation for training this additional model is to show that the deformations learned by our method can be approximated using a system with a significantly lower capacity. Such a simple regression model can be easier to apply for inference on low-performance devices.

To train the linear model, we first obtain the basis of offsets 
ℱ
∈
ℝ
3
​
�
×
�
, which is similar to the blendshapes used in the FLAME parametric model. This basis is obtained by applying a low-rank PCA [47] to the matrix of offsets 
Δ
​
𝐕
^
∈
ℝ
3
​
�
×
�
, calculated using 
�
 images from the dataset. Following [25], we discard most of the basis vectors and only keep 
�
 components corresponding to maximal singular values. The approximated vertex offsets 
𝐯
~
 for each image can then be estimated as following 
𝐯
~
=
ℱ
​
�
, where 
�
 is obtained by applying the pseudo-inverse of a basis matrix 
ℱ
 to the corresponding offsets 
Δ
​
𝐯
^
: 
�
=
(
ℱ
�
​
ℱ
)
−
1
​
ℱ
�
​
Δ
​
𝐯
^

We then train the regression network by estimating a vector of basis coefficients 
�
�
, given an image 
𝐱
�
. For that, we minimize the mean squared error (MSE) loss 
‖
�
^
�
−
�
�
‖
2
2
 between the estimated coefficients and the ground truth, as well as the segmentation loss 
ℒ
occ
 and a Chamfer distance between predicted and ground truth meshes.

Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	
Source	H3D-Net	ROME	GT	H3D-Net	ROME	
Figure 3:Qualitative comparison of the representative cases from the H3DS dataset. While neither of the two methods achieves perfect results, arguably, ROME achieves more realistic renders and better matches the head geometry than H3D-Net in the single-shot mode. Furthermore, an important advantage of ROME is that the resulting avatars are ready for animation and are obtained in a feed-forward manner without the lengthy fine-tuning process employed by H3D-Net.
4Experiments
We train our models on the VoxCeleb2 [5] dataset of videos. This large-scale dataset contains an order of 
10
5
 videos of 
10
3
 different speakers. It is widely used [48, 8, 11] to train talking head models. However, the main drawback of this dataset is the mixed quality of videos and the heavy bias towards frontal poses.

To address these well-known limitations, we process this dataset using an off-the-shelf image quality analysis model [49] and a 3D face-alignment network [50]. We then filter out the data which has poor quality and non-diverse head rotations. Our final training dataset has 
≈
15000
 sequences. We note that filtering/pruning does not fully solve the problem of head rotation bias, and our method still works best in frontal views. For more details, please refer to the supplementary materials.

We also use the H3DS [20] dataset of photos with associated 3D scans to evaluate the quality of head reconstructions.

4.1Implementation details
In the experiments, unless noted otherwise, we train all architectures jointly and end-to-end. We use the following weights: 
�
hair
=
10
, 
�
per
=
1
, 
�
idt
=
0.1
, 
�
adv
=
0.1
, 
�
seg
=
10
, and enable the neck and the 2D Chamfer loss 
�
chm
=
0.01
) and 
�
lap
=
10
. We ablate all geometry losses and method parts below.

We train our models at 
256
×
256
 resolution using ADAM [51] with the fixed learning rate of 
10
−
4
, 
�
1
=
0
, 
�
2
=
0.999
, and a batch size of 
32
. For more details, please refer to the supplementary materials.

4.2Evaluation
Table 1:Evaluation results on the H3DS dataset in the one-shot scenario for our models, H3D-Net, and DECA. We compute Chamfer distance (lower is better) across all available scans, reconstructed from three different viewpoints. Both of the ROME variants significantly exceed H3D-Net in the one-shot reconstruction quality.
Method	DECA	H3D-Net	ROME	LinearROME
Chamfer Distance	
15.0
15.1
12.6
12.5
3D reconstruction. We evaluate our head reconstruction quality using a novel H3DS dataset [20]. We compare against the state-of-the-art head reconstruction method H3D-Net [20], which uses signed distance functions to represent the geometry. While providing great reconstruction quality in the sparse-view scenario, their approach has several limitations. For example, H3D-Net requires a dataset of full head scans to learn the prior on head shapes. Additionally, its results do not have fixed topology or rigging and their method requires fine-tuning per scene, while our method works in a feed-forward way.

We carry out the comparison with H3D-Net in a single-view scenario, which is native for our method but is beyond the capabilities stated by the authors in the original publication [20]. However, to the best of our knowledge, H3D-Net is the closest system to ours in single-view reconstruction capabilities (out of all systems with either their code or results available). Additionaly, we tried to compare our system with PIFuHD [52], which unfortunately failed to work with heads images without body (see supplementary).

We show qualitative comparison in Figure 3. We evaluate our method and H3D-Net both for frontal- and side-view reconstruction. We note the significant overfitting of H3D-Net to the visible hair geometry, while our model provides reconstructions more robust to the change of viewpoint.

In total, we compared our models on all scans available in the test set of the H3DS dataset, and each scan was reconstructed from three different viewpoints. We provide the measured mean Chamfer distance both for our method and baselines across all scans in Tab. 1.

Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Source	Driver	FOMM	Bi-Layer	FLAMETex	ROME
Figure 4:Comparison of renders on a VoxCeleb2 dataset. The task is to reenact the source image with the expression and pose of the driver image. Here, we picked diverse examples in terms of pose variation to highlight the differences in performance of compared methods. We observe that for the large head pose rotations, purely neural-based methods (FOMM, Bi-Layer) struggle to maintain consistent quality. In contrast, our rendering method produces images that are more robust to pose changes. Admittedly, for small pose changes, neural-based methods exhibit a smaller identity gap than ROME (bottom row) and overall outperform our method in terms of rendering quality. As a reference, we also include a non-neural FLAMETex rendering method, which is employed in state-of-the-art one-shot face reconstruction systems [2] but is not able to personalize the avatar at the head level.
Rendering. We evaluate the quality of our renders on the hold-out subset VoxCeleb2 dataset. We use a cross-driving comparison scenario for qualitative comparison to highlight the animation capabilities of our method, and self-driving scenario for quantitative comparison.

First, we compare with a FLAMETex [25] rendering system, which works explicitly with mesh rendering. From the source image, FLAMETex estimates the albedo via a basis of RGB textures, and then combines it with predicted scene-specific shading. In contrast, our method predicts a rendered image directly and avoids the complexity of explicit albedo-shading decomposition.

We then compare with publicly available geometry-free rendering methods, which were trained on the same dataset. For that, we use the First-Order Motion Model (FOMM) [9], the Bi-Layer Avatar Model [11] and recently proposed Thin-Plate-Spline-Motion-Mode (TPSMM) [53]. Both these systems bypass explicit 3D geometry estimation and rely only on learning the scene structure via the parameters of generative ConvNets. Other methods [8, 48], which internally utilize some 3D structures, like camera rotations, were out of the scope of our comparison due to the unavailability of pre-trained models.

Table 2:Here we present the quantitative results on the VoxCeleb2-HQ dataset in the self-reenactment and cross-reenactment modes. Our ROME system performs on par with FOMM and TPSMM in self-reenactment, notably outperforming them in the most perceptually-plausible LPIPS metrics. On the contrary, in the cross-driving scenario, when the task is complex for pure neural-based systems, our method obtains better results.
self-reenactment		cross-reenactment
Method	LPIPS
↓
SSIM
↑
PSNR
↑
FID
↓
CSIM
↑
IQA
↑
FOMM	
0.09
0.87
25.8
52.95
0.53
55.9
Bi-Layer	
0.08
0.83
23.7
51.4
0.56
50.48
TPSMM	
0.09
0.85
26.1
49.27
0.57
59.5
ROME	
0.08
0.86
26.2
45.32
0.62
66.3
We present the qualitative comparison in Figure 4, and a quantitative comparison across a randomly sampled hold-out VoxCeleb2 subset in Table 2. We restrict the comparison to the face and hair region as the shoulder pose is not controlled by our method (driven by DECA parameters), which is admittedly a limitation of our system. We thus mask the results according to the face and hair mask estimated from the ground truth image.

Generally, we observe that over the entire test set, the quality of ROME avatars in the self-reenactment mode is similar to FOMM and better than the Bi-layer model. For the cross-reenactment scenario, our model is clearly better than both alternatives according to three metrics, that help to asses unsupervised quality of the images in three aspects: realism, identity preservation and blind quality of the image. The huge gap for IQA [49] and FID [54] is also noticeable in the qualitative comparison, especially for strong pose change (see CSIM [10] column in Tab. 2). The PSNR and SSIM metrics penalize slight misalignments between the sharp ground truth and our renderings much stronger than the blurriness in FOMM reconstructions. The advatage of ROME avatar is noticable even for self-driving case according to LPIPS. We provide a more extensive qualitative evaluation in the supplementary materials.

4.3Linear basis experiments
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Input	Full	w/o 
Δ
​
𝐯
^
w/o 
�
→
w/o 
ℒ
occ
w/o 
ℒ
lap
w/o 
ℒ
chm
Figure 5:Ablation study. We qualitatively evaluate the individual components of our full model. w/o 
Δ
​
𝐯
^
: without the per-vertex displacements, we obtain a significantly worse render quality. w/o 
�
→
: when we apply per-vertex deformations instead of per-vertex displacements (i.e., deformations alongside the normals), we obtain noisy reconstructions in neck area and worse renders. w/o 
ℒ
occ
: without silhouette-based losses, our model fails to learn proper reconstructions. w/o 
ℒ
lap
: Laplacian regularization smooths the reconstructions. w/o 
ℒ
chm
: chamfer loss allows us to constrain the displaced vertices to lie inside the scene boundaries, which positively affects the smoothness of the visible part of the reconstruction.
As discussed above, we distill our ROME head reconstruction model into a linear parametric model. To do that, we set the number of basis vectors to 
50
 for the hair and 
10
 for the neck offsets and run low-rank Principle Component Analysis (PCA) to estimate them. The number of components is chosen to obtain a low enough approximation error. Interestingly, the offsets learned by our model can be compressed by almost two orders of magnitude in terms of degrees of freedom without any practical loss in quality (Figure 66(a)), which suggests that the capacity of the offset generator is underused in our model. We combine estimated basis vectors with the original basis of the FLAME.

After that, we train feed-forward encoders that directly predict the coefficients of the two basis from the source image. The prediction is performed in two stages. First, face expression, pose and camera parameters are predicted with a MobileNetV2 [55] encoder. Then a slower ResNet-50 encoder [56] is used to predict hair, neck and shape coefficients. The choice of architectures are motivated by the fact that in many practical scenarios only the first encoder needs to be invoked frequently (per-frame), while the second can run at much lower rate or even only at the model creation time.

4.4Ablation study
We demonstrate results of ablation study at Figure 5. As expected, predicting more accurate geometry affect the renders (first row). Also, we verify the necessity of all terms of geometry loss. We observe significant improvement in quality of renders with additional geometry (see Supp 3), which leads us to an optimistic conclusion that our learned coarse mesh may be integrated into other neural rendering systems [8] to improve their quality. Additionally, we observe that geometry losses allows to correctly model coarse details on the hair without noise and reconstruct the hair without sticking with neck. Similar artifacts are removed by adding shifts along the normals.

Refer to caption	Refer to caption	Refer to caption
Image	ROME	Distilled
(a)Linear model
Refer to caption	Refer to caption	Refer to caption
Long hair	Ear cover	Failed renders
(b)Limitations
Figure 6:Linear model results and the examples of limitations. On the left, we show how reconstructions learned by our method, ROME, could be distilled using a linear parametric model. We are able to compress the learned offsets into a small basis, reducing the degrees of freedom by two orders of magnitude. We can then distill these offsets using a much faster regression network with a small gap in terms of the reconstruction quality. On the right, we highlight the main limitations of our method, which include the failure related to long hair modelling, caused by an incorrect topological prior, no coverage of ears and unrealistic renders under a significant change of scales.
4.5Limitations
In some cases, the proposed system often produces somewhat oversmoothed geometry without person-specific attributes. Most of those attributes simply cannot be presented with the limited set of vertices. Our preliminary experiments with subdivision during training did not help to alleviate this problem. It may be interesting to study this problem in a few-shot scenario (given different views) or to use images and proxy geometry in a higher resolution. Another frequent geometry artifact is the roughness of the clothing approximation. As we know, modeling any clothes without 3D data is an extremely difficult task and the fact that our models is not able to do so from videos is not surprising. A principled solution to this problem will likely require learning from datasets that cover these areas (e.g.,  circular 3D scans of human heads) [57].

Our current model is trained at roughly fixed scale, though explicit geometry modeling allows it to generalize to adjacent scale reasonably well. Still, strong changes of scale lead to poor performance (Figure 66(b)). More examples are provided in the supplementary materials. Addressing this issue via mip-mapping and multi-scale GAN training techniques remains future work.

Lastly, our model can have artifacts with long hair (Figure 66(b), left) or ears (Figure 66(b), middle). Handling such cases gracefully are likely to require a departure from the predefined FLAME mesh connectivity to new person-specific mesh topology. Handling such issues using a limited set of pre-designed hair meshes is an interesting direction for future research.

5Summary
We have presented ROME avatars: a system for creating realistic one-shot mesh-based human head models that can be animated and are compatible with FLAME head models. We compare our model with representative state-of-the-art models from different classes, and show that it is highly competitive both in terms of geometry estimation and the quality of rendering.

Crucially, our system can learn to model head geometry without direct supervision in the form of 3D scans. Despite that, we have observed it to achieve state-of-the-art results in head geometry recovery from a single photograph. At the same time, it also performs better than previous one-shot neural rendering approaches in the cross- and self-driving scenario. We have thus verified that the resulting geometry could be used to improve the rendering quality.

As neural rendering becomes more widespread within graphics systems, ROME avatars and similar systems can become directly applicable, while their one-shot capability and the simplicity of rigging derived from DECA and FLAME could become especially important in practical applications.

Acknowledgements
We sincerely thank Eduard Ramon for providing us the one-shot H3D-Net reconstructions. We also thank Arsenii Ashukha for their comments and suggestions regarding the text contents and clarity, as well as Julia Churkina for helping us with proof-reading. The computational resources for this work were mainly provided by Samsung ML Platform.

References
[1]Volker Blanz and Thomas Vetter.A morphable model for the synthesis of 3d faces.In SIGGRAPH ’99, 1999.
[2]Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart.Learning an animatable detailed 3d face model from in-the-wild images.ACM Transactions on Graphics (TOG), 40:1 – 13, 2020.
[3]Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan Z Li.Towards fast, accurate and stable 3d dense face alignment.In Proceedings of the European Conference on Computer Vision (ECCV), 2020.
[4]Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J. Black.Learning to regress 3d face shape and expression from an image without 3d supervision.2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7755–7764, 2019.
[5]Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.Voxceleb2: Deep speaker recognition.In INTERSPEECH, 2018.
[6]Justus Thies, Michael Zollhöfer, and Matthias Nießner.Deferred neural rendering: Image synthesis using neural textures.arXiv: Computer Vision and Pattern Recognition, 2019.
[7]Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari.Accelerating 3d deep learning with pytorch3d.arXiv:2007.08501, 2020.
[8]Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu.One-shot free-view neural talking-head synthesis for video conferencing.2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10034–10044, 2021.
[9]Aliaksandr Siarohin, Stéphane Lathuilière, S. Tulyakov, Elisa Ricci, and N. Sebe.First order motion model for image animation.Advances in Neural Information Processing Systems (NeurIPS), 2019.
[10]Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor S. Lempitsky.Few-shot adversarial learning of realistic neural talking head models.In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
[11]Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, and Victor S. Lempitsky.Fast bi-layer neural synthesis of one-shot realistic head avatars.In ECCV, 2020.
[12]J. Thies, M. Zollhöfer, M. Stamminger, C. Theobalt, and M. Nießner.Face2face: Real-time face capture and reenactment of rgb videos.In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2016.
[13]Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nießner, Patrick Pérez, Christian Richardt, Michael Zollöfer, and Christian Theobalt.Deep video portraits.ACM Transactions on Graphics (TOG), 37(4):163, 2018.
[14]AvatarSDK.https://avatarsdk.com/.
[15]Pinscreen.https://www.pinscreen.com/.
[16]Tal Hassner, Shai Harel, Eran Paz, and Roee Enbar.Effective face frontalization in unconstrained images.2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4295–4304, 2015.
[17]Volker Blanz, Sami Romdhani, and Thomas Vetter.Face identification across different poses and illuminations with a 3d morphable model.Proceedings of Fifth IEEE International Conference on Automatic Face Gesture Recognition, pages 202–207, 2002.
[18]A. Tran, Tal Hassner, Iacopo Masi, and Gérard G. Medioni.Regressing robust and discriminative 3d morphable models with a very deep neural network.2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1493–1502, 2017.
[19]Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.Nerf: Representing scenes as neural radiance fields for view synthesis.In ECCV, 2020.
[20]Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia Giraldez, Xavier Giró i Nieto, and Francesc Moreno-Noguer.H3d-net: Few-shot high-fidelity 3d head reconstruction.Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.
[21]Stephen Lombardi, Jason M. Saragih, Tomas Simon, and Yaser Sheikh.Deep appearance models for face rendering.ACM Transactions on Graphics (TOG), 37:1 – 13, 2018.
[22]Stephen Lombardi, Tomas Simon, Jason M. Saragih, Gabriel Schwartz, Andreas M. Lehrmann, and Yaser Sheikh.Neural volumes.ACM Transactions on Graphics (TOG), 38:1 – 14, 2019.
[23]Bernhard Egger, W. Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhöfer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, and Thomas Vetter.3d morphable face models—past, present, and future.ACM Transactions on Graphics (TOG), 39:1 – 38, 2020.
[24]Stylianos Ploumpis, Evangelos Ververas, Eimear O’ Sullivan, Stylianos Moschoglou, Haoyang Wang, Nick E. Pears, W. Smith, Baris Gecer, and Stefanos Zafeiriou.Towards a complete 3d morphable model of the human head.In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
[25]Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero.Learning a model of facial shape and expression from 4d scans.ACM Transactions on Graphics (TOG), 36:1 – 17, 2017.
[26]Shunsuke Saito, Tomas Simon, Jason M. Saragih, and Hanbyul Joo.Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization.2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81–90, 2020.
[27]Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason M. Saragih.Mixture of volumetric primitives for efficient neural rendering.ACM Transactions on Graphics (TOG), 40:1 – 13, 2021.
[28]Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, and Michael J. Black.Scale: Modeling clothed humans with a surface codec of articulated local elements.In CVPR, 2021.
[29]Jeong Joon Park, Peter R. Florence, Julian Straub, Richard A. Newcombe, and S. Lovegrove.Deepsdf: Learning continuous signed distance functions for shape representation.2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 165–174, 2019.
[30]Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan P. Spicer, Kari Pulli, and Gordon Wetzstein.Neural lumigraph rendering.2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
[31]Michael Oechsle, Songyou Peng, and Andreas Geiger.Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction.International Conference on Computer Vision (ICCV), 2021.
[32]Philip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias Nießner, and Justus Thies.Neural head avatars from monocular rgb videos.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
[33]Ilya Zakharkin, Kirill Mazur, Artur Grigoriev, and Victor S. Lempitsky.Point-based modeling of human clothing.2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021.
[34]Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed A. Elgharib, Daniel Cremers, and Christian Theobalt.i3dmm: Deep implicit 3d morphable model of human heads.2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12798–12808, 2021.
[35]Yufeng Zheng, Victoria Fernández Abrevaya, Marcel C. Bühler, Xu Chen, Michael J. Black, and Otmar Hilliges.I m avatar: Implicit morphable head avatars from videos.In 2022 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
[36]Guy Gafni, Justus Thies, Michael Zollhöfer, and Matthias Nießner.Dynamic neural radiance fields for monocular 4d facial avatar reconstruction.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
[37]Shichen Liu, Tianye Li, Weikai Chen, and Hao Li.Soft rasterizer: A differentiable renderer for image-based 3d reasoning.2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 7707–7716, 2019.
[38]Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila.Modular primitives for high-performance differentiable rendering.ACM Transactions on Graphics, 39(6), 2020.
[39]Olaf Ronneberger, Philipp Fischer, and Thomas Brox.U-net: Convolutional networks for biomedical image segmentation.In MICCAI, 2015.
[40]Olga Sorkine-Hornung.Laplacian mesh processing.In Eurographics, 2005.
[41]Justin Johnson, Alexandre Alahi, and Li Fei-Fei.Perceptual losses for real-time style transfer and super-resolution.In ECCV, 2016.
[42]Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman.Vggface2: A dataset for recognising faces across pose and age.2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), pages 67–74, 2018.
[43]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.Generative adversarial nets.In NIPS, 2014.
[44]Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.High-resolution image synthesis and semantic manipulation with conditional gans.2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8798–8807, 2018.
[45]Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.V-net: Fully convolutional neural networks for volumetric medical image segmentation.2016 Fourth International Conference on 3D Vision (3DV), pages 565–571, 2016.
[46]Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J. Black.Three-d safari: Learning to estimate zebra pose, shape, and texture from images ”in the wild”, 2019.
[47]Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp.Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions.SIAM Rev., 53:217–288, 2011.
[48]Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia Sharmanska.Headgan: Video-and-audio-driven talking head synthesis.2021.
[49]Shaolin Su, Qingsen Yan, Yu Zhu, Cui cui Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang.Blindly assess image quality in the wild guided by a self-adaptive hyper network.2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
[50]Adrian Bulat and Georgios Tzimiropoulos.How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks).In International Conference on Computer Vision, 2017.
[51]Diederik P. Kingma and Jimmy Ba.Adam: A method for stochastic optimization.International Conference for Learning Representations, 2015.
[52]Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo.Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2020.
[53]Jian Zhao and Hui Zhang.Thin-plate spline motion model for image animation.2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
[54]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilibrium.In Advances in Neural Information Processing Systems, 2017.
[55]Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.Mobilenetv2: Inverted residuals and linear bottlenecks.2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018.
[56]Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition.2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
[57]Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard Pons-Moll.Learning to reconstruct people in clothing from a single rgb camera.In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[58]Karen Simonyan and Andrew Zisserman.Very deep convolutional networks for large-scale image recognition.International Conference on Learning Representations, 2015.
[59]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei.Imagenet large scale visual recognition challenge.International Journal of Computer Vision, 2015.
[60]Kevin Cortacero, Tobias Fischer, and Y. Demiris.Rt-bene: A dataset and baselines for real-time blink estimation in natural environments.2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 1159–1168, 2019.
[61]Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.Spectral normalization for generative adversarial networks.ArXiv, abs/1802.05957, 2018.
[62]Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro.Few-shot video-to-video synthesis.Advances in Neural Information Processing Systems (NeurIPS), 2019.
[63]Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros.Image-to-image translation with conditional adversarial networks.2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967–5976, 2017.
[64]Yuxin Wu and Kaiming He.Group normalization.In ECCV, 2018.
[65]Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Loddon Yuille.Weight standardization.ArXiv, abs/1903.10520, 2019.
[66]Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.Instance normalization: The missing ingredient for fast stylization.ArXiv, abs/1607.08022, 2016.
[67]Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun.Identity mappings in deep residual networks.ECCV, 2016.
[68]Tero Karras, Samuli Laine, and Timo Aila.A style-based generator architecture for generative adversarial networks.2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4396–4405, 2019.
[69]Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael Black.Collaborative regression of expressive bodies using moderation.In International Conference on 3D Vision (3DV), 2021.
Appendix 0.ASupplementary material
0.A.1Implementation details
Photometric training objectives.
During training, we use the photometric loss 
ℒ
photo
 to aid in learning the geometry, as well as to train the rendering. Our photometric loss is the combination of the perceptual, the identity, the adversarial, and the segmentation losses:

ℒ
photo
=
�
per
​
ℒ
per
+
�
idt
​
ℒ
idt
+
�
adv
​
ℒ
adv
+
�
seg
​
ℒ
seg
.
(1)
For the perceptual loss 
ℒ
per
, we use a weighted combination of distances between features of predicted and target images. These features are taken from two pre-trained convolutional neural networks. We use features from a VGG19 [58] network pre-trained on ImageNet [59] to match the general content and features from a VGG16-based gaze detection network [60] to match the gaze direction. This loss, therefore, can be expressed as follows:

ℒ
per
=
ℒ
vgg
+
ℒ
gaze
.
(2)
In both of these losses, we measure the L1 distance between conv1_1, conv2_1, conv3_1, conv4_1, and conv5_1 features after ReLU activations. We then sum these distances with the following weights: 
1
32
,
1
16
,
1
8
,
1
4
,
 and 
1
 to obtain the loss value.

In the gaze detection network, we independently process both eyes and average the losses corresponding to each one of them. Before feature extraction, we additionally perform alignment of both eyes using the keypoints extracted from the image via a differentiable interpolation. For more details, please refer to [60].

The face identity loss 
ℒ
idt
 is the cosine distance between the embeddings of a pre-trained face recognition network [42], evaluated for the predicted and the target images. We use this loss to better preserve the person’s identity in the renders. We calculate the cosine distance as minus the cosine similarity.

To calculate the adversarial loss 
ℒ
adv
 we jointly train a discriminator network alongside our reconstruction and rendering networks. We use a weighted combination of the hinge-loss discriminator loss as well as the feature-matching loss [44]. We additionally apply spectral normalization [61] to the discriminator network. The configuration of the adversarial training closely follows the related works on image-to-image translation [62]. In particular, we use the PatchGAN [63] architecture for the discriminator.

Lastly, we use Dice loss to match the predicted segmentation 
𝐬
^
�
 to the ground-truth 
𝐬
�
:

ℒ
seg
=
1
−
2
​
𝐬
^
�
⋅
𝐬
�
‖
𝐬
^
�
‖
2
2
+
‖
𝐬
�
‖
2
2
,
(3)
where 
⋅
 denotes a scalar product. The segmentation loss is calculated using each batch element separately and then averaged across the batch.

Architectures of the neural networks.
We use group normalization [64] paired with weight standardization [65] in all networks to facilitate training with smaller batch sizes. Empirically we found this combination to perform better than standard instance normalization [66].

In the autoencoder 
�
tex
, which encodes the input image 
𝐱
�
 into the neural texture 
𝐓
�
, we use pre-activation residual blocks [67]. We set the number of channels in the neural texture 
𝐓
�
 to eight (we observed that values between 
8
 and 
16
 result in similar performance). We additionally align the input image using the transformation similar to the one used in the FFHQ dataset [68]. We only modify the zoom-out factor to 
1.25
 so that the aligned image contains more hair and upper-body regions.

For the networks 
�
img
 and 
�
geom
 we use a standard U-Net architecture. The network 
�
img
 is U-Net, which predict an output image and the segmentation mask. Besides the neural texture, we additionally condition these two networks on the rendered mesh normals. Specifically, we process each input dimension of the normal vectors using 
{
sin
⁡
(
�
​
�
)
}
�
=
1
�
 and 
{
cos
⁡
(
�
​
�
)
}
�
=
1
�
 functions. In our experiments, we set 
�
=
6
. We then concatenate the resulting encodings to the neural texture. We use max-pooling layers for downsampling and nearest neighbors upsampling in the network, which decodes an image, and average pooling with bilinear upsampling in the network, which decodes segmentations. Additionally, the segmentation U-Net network has two times fewer channels than the image network.

The architecture of 
�
geom
 is the same as the image encoding U-Net, albeit with a different number of input and output channels. We set the dimensionality of a latent geometry map 
𝐙
�
, which is an output of 
�
geom
, to 32. Additionally, we encode the 
�
​
�
​
�
-texture using harmonic functions via the same process described before. We then concatenate the obtained embeddings to the initial 
�
​
�
​
�
-texture. The resulting feature map has 
3
+
3
⋅
6
⋅
2
=
39
 channels. In total, 
�
geom
 has 
8
+
39
=
47
 input channels, since we also concatenate the embeddings of the 
�
​
�
​
�
-texture to the neural texture.

Finally, 
�
geom
 consists of an MLP network which we apply separately for each vertex to predict its offsets. To obtain its inputs, we first resample the latent geometry map 
𝐙
�
 using an irregular grid specified by the texture coordinates 
�
. The obtained features for each vertex are denoted as 
𝐳
�
�
. These features are concatenated to the harmonic embeddings of 
�
 in the same way as the 
�
​
�
​
�
-texture. Specifically, each vector 
�
 has two dimensions, therefore we encode it into 
2
+
2
⋅
6
⋅
2
=
26
 features, and then concatenate with 
32
 channels of 
𝐳
�
�
 to obtain a vector with the dimensionality of 
58
 — the number of 
�
geom
 input dimensions.

Training details.
We use the ADAM optimizer to train all networks in our model. We set the learning rate to 
1
⋅
10
−
4
 for all the networks except for the discriminator. For it, we set the learning rate to 
4
⋅
10
−
4
. We also set 
�
1
=
0
, and 
�
2
=
0.999
. We train using eight P40 NVIDIA GPUs to facilitate the batch size of 32. We observe that decreasing the batch size leads to visible degradation of both rendering and reconstructions, but we did no ablations to measure this effect.

0.A.2Evaluation
We present additional results for the 3D reconstruction in both self-driving (reconstruction using frames from the same video), and cross-driving (reconstruction from a photo of one person and animation from a video of a different person) scenarios. In Figure 8, we present more self-driving results on the H3DS dataset, as well as a side-by-side comparison with H3D-Net. In Figure 9, we present cross-driving evaluation results using the hold-out samples from the VoxCeleb2 dataset, as well as paintings, the latter allowing us to evaluate the susceptibility of our approach to domain shifts.

Refer to caption	Refer to caption	Refer to caption
PIFuHD
Full body  	 
Ours
Head 	 
PIFuHD
Head 
(a)Comparison with PIFuHD.
Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption
Source
Image  	 
ROME
w/ DECA 	 
ROME(L)
w/ PIXIE 
(b)ROME integrated with PIXIE.
Additionally, we show the results in Fig. 7(a). PIFuHD can only recover head geometry from the full-body images, which prevents us from doing a comparison with this method using existing head reconstruction and reenactment benchmarks. While having more detailed reconstructions, PIFuHD requires training with 3D supervision and does not have animation capabilities. Currently, these limitations can be lifted only at the cost of lengthy multi-shot training per each avatar (IMAvatars), or by sacrificing some detalization of reconstructions and retaining both real-time and one-shot capabilities, which is done in our method.

To demonstrate the the ease of integration with existing SMPL-X based models we predict the vertices corresponded to hair using distilled version of the ROME with PIXIE [69]. The resulted mesh contains the hair from ROME basis and shoulders from SMPL-X.

We provide an extended cross-driving qualitative comparison with neural-based rendering methods in Figure 10. Then, we provide an additional self-driving qualitative comparison in Figure 11. Most of the results are consistent with the metrics obtained in the main text.

Method	LPIPS
↓
SSIM
↑
PSNR
↑
w/o 
Δ
​
𝐯
^
 	
0.10
0.81
23.1
ROME	
0.08
0.86
25.8
Table 3:Quantitative ablation on a hold-out set of the VoxCeleb2 dataset. We observe that the deferred neural rendering trained without offsets achieves lower image quality for face and hair regions, than a full ROME system.
We evaluate the effect that trained offsets have on the quality of rendering. To do that, we remove the head reconstruction step from the training pipeline and only train deferred neural rendering system using base FLAME meshes. The quantitative results are in Table 3. Notice how the quality degrades for the model with no offsets in the hair and shoulders areas and to the overall worse quantitatively measured performance. This leads us to the conclusion that our system for coarse mesh estimation can aid other neural rendering systems produce better quality reconstructions.

0.A.3Linear model
We evaluate visual quality in Figure 13. We note that the value of MSE that is achieved by our regressor leads to visually similar reconstructions. Our mesh reconstruction model can achieve up to 10 times speed-up without any perceived degradation in reconstruction quality.

Additionally, we evaluate the semantic manipulation capabilities of the linear model in a similar way to the face parametric models. Specifically, we pick individual basis vectors and see how varying their coefficient changes the reconstructed mesh. The results can be seen in Figure 12. We observe a certain semantic disentanglement for the first hair and neck basis vectors. This disentanglement allows us to perform mesh and image editing tasks, like the editing of hairstyle, which we show in Figure 14. Here, we obtain this modified reconstruction by simply varying one of the predicted coefficients for the linear basis.

Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Source	H3D-Net	ROME	Target	H3D-Net	ROME
Figure 8:Extended qualitative comparison on the H3DS dataset. We compare 3D reconstructions and renders obtained using a single source image.
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Source	Driver	ROME	   Source	Driver	ROME
Figure 9:Additional examples of mesh-based avatars creating using a single source image, and animated using the camera pose and the expression parameters estimated from the driver image. First five rows contains results for samples out of the train distribution.
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Source	Driver	FOMM	Bi-Layer	ROME
Figure 10:Additional comparison of renders on a VoxCeleb2 dataset. The task is to reenact the source image with the expression and pose of the driver image. This comparison is done in a cross-driving scenario, which we also use for quantitative comparison.
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Source	Driver	FOMM	Bi-Layer	ROME
Figure 11:Additional comparison of renders on a VoxCeleb2 dataset. The task is to reenact the source image with the expression and pose of the driver image. This comparison is done in a self-driving scenario, which we also use for quantitative comparison.
1	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
2	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
4	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
52	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
�
�
(
1
)
�
�
(
⌊
0.1
​
�
⌋
)
�
�
(
⌊
0.25
​
�
⌋
)
�
�
(
⌊
0.75
​
�
⌋
)
�
�
(
⌊
0.9
​
�
⌋
)
�
�
(
�
)
�
Figure 12:We show how the estimated meshes can be semantically manipulated by varying the individual components of the PCA basis. We assume that 
�
 meshes were initially predicted by the ROME system. Then, we estimate 
�
 coefficients 
�
�
∈
ℝ
�
, each is used to reconstruct 
�
-th mesh via the PCA basis. In our experiments, the set of 50 basis vectors are used to reconstruct the hair, and the set of 10 basis vectors are used to reconstruct the neck and the shoulders. We denote each component of 
�
�
 as 
�
�
�
. Then, we use the mean vector 
�
=
1
�
​
�
�
 to reconstruct the base mesh, and modify its 
�
-th component to the 
�
-th order statistic 
�
(
�
)
�
 over the dataset 
{
�
�
�
}
�
=
1
�
, where 
�
∈
{
1
,
…
,
�
}
. We show the resulting reconstructions above. Each row corresponds to a component which is modified, and each column corresponds to its new value (minimum, 10%, 25%, 75%, 90%, maximum). The first three lines correspond to the hair components, and the last line – to the neck component.
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	  Refer to caption	Refer to caption	Refer to caption	Refer to caption
Image	ROME	PCA	Distilled	   Image	ROME	PCA	Distilled
Figure 13:We provide more examples to qualitatively evaluate the performance of the distilled linear model.
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption	Refer to caption
Image	Original	Edit 1	Edit 2	Edit 4	Edit 5
Figure 14:Examples of hair style manipulation using only individual components of a PCA basis.
◄ ar5iv homepage Feeling
lucky? Conversion
report Report
an issue View original
on arXiv►
Copyright Privacy Policy Generated on Mon Mar 11 16:36:03 2024 by LaTeXMLMascot Sammy